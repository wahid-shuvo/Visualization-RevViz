{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3b35047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--algo ALGO] [--repeat REPEAT]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\wahid\\AppData\\Roaming\\jupyter\\runtime\\kernel-cc941f19-aef5-42e4-a5a9-4f0065569810.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wahid\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3449: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import  precision_score\n",
    "from sklearn.metrics import  f1_score\n",
    "\n",
    "import  random\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from xlrd import open_workbook\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "def replace_all(text, dic):\n",
    "    for i, j in dic.iteritems():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "stemmer =SnowballStemmer(\"english\")\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens)\n",
    "    return stems\n",
    "\n",
    "mystop_words=[\n",
    "'i', 'me', 'my', 'myself', 'we', 'our',  'ourselves', 'you', 'your',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'themselves',\n",
    " 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the',\n",
    "'and',  'if', 'or', 'as', 'until',  'of', 'at', 'by',  'between', 'into',\n",
    "'through', 'during', 'to', 'from', 'in', 'out', 'on', 'off', 'then', 'once', 'here',\n",
    " 'there',  'all', 'any', 'both', 'each', 'few', 'more',\n",
    " 'other', 'some', 'such',  'than', 'too', 'very', 's', 't', 'can', 'will',  'don', 'should', 'now'\n",
    "# keywords\n",
    " 'while', 'case', 'switch','def', 'abstract','byte','continue','native','private','synchronized',\n",
    " 'if', 'do', 'include', 'each', 'than', 'finally', 'class', 'double', 'float', 'int','else','instanceof',\n",
    " 'long', 'super', 'import', 'short', 'default', 'catch', 'try', 'new', 'final', 'extends', 'implements',\n",
    " 'public', 'protected', 'static', 'this', 'return', 'char', 'const', 'break', 'boolean', 'bool', 'package',\n",
    " 'byte', 'assert', 'raise', 'global', 'with', 'or', 'yield', 'in', 'out', 'except', 'and', 'enum', 'signed',\n",
    " 'void', 'virtual', 'union', 'goto', 'var', 'function', 'require', 'print', 'echo', 'foreach', 'elseif', 'namespace',\n",
    " 'delegate', 'event', 'override', 'struct', 'readonly', 'explicit', 'interface', 'get', 'set','elif','for',\n",
    " 'throw','throws','lambda','endfor','endforeach','endif','endwhile','clone'\n",
    "]\n",
    "\n",
    "#logging.basicConfig(level=logging.INFO,\n",
    "#                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "\n",
    "emodict=[]\n",
    "contractions_dict=[]\n",
    "\n",
    "\n",
    "# Read in the words with sentiment from the dictionary\n",
    "with open(\"Contractions.txt\",\"r\") as contractions,\\\n",
    "     open(\"EmoticonLookupTable.txt\",\"r\") as emotable:\n",
    "    contractions_reader=csv.reader(contractions, delimiter='\\t')\n",
    "    emoticon_reader=csv.reader(emotable,delimiter='\\t')\n",
    "\n",
    "    #Hash words from dictionary with their values\n",
    "    contractions_dict = {rows[0]:rows[1] for rows in contractions_reader}\n",
    "    emodict={rows[0]:rows[1] for rows in emoticon_reader}\n",
    "\n",
    "    contractions.close()\n",
    "    emotable.close()\n",
    "\n",
    "grammar= r\"\"\"\n",
    "NegP: {<VERB>?<ADV>+<VERB|ADJ>?<PRT|ADV><VERB>}\n",
    "{<VERB>?<ADV>+<VERB|ADJ>*<ADP|DET>?<ADJ>?<NOUN>?<ADV>?}\n",
    "\n",
    "\"\"\"\n",
    "chunk_parser = nltk.RegexpParser(grammar)\n",
    "\n",
    "\n",
    "contractions_regex = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "     def replace(match):\n",
    "         return contractions_dict[match.group(0)]\n",
    "     return contractions_regex.sub(replace, s.lower())\n",
    "\n",
    "\n",
    "url_regex = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "def remove_url(s):\n",
    "    return url_regex.sub(\" \",s)\n",
    "\n",
    "negation_words =['not', 'never', 'none', 'nobody', 'nowhere', 'neither', 'barely', 'hardly',\n",
    "                     'nothing', 'rarely', 'seldom', 'despite' ]\n",
    "\n",
    "emoticon_words=['PositiveSentiment','NegativeSentiment']\n",
    "\n",
    "def negated(input_words):\n",
    "    \"\"\"\n",
    "    Determine if input contains negation words\n",
    "    \"\"\"\n",
    "    neg_words = []\n",
    "    neg_words.extend(negation_words)\n",
    "    for word in neg_words:\n",
    "        if word in input_words:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def prepend_not(word):\n",
    "    if word in emoticon_words:\n",
    "        return word\n",
    "    elif word in negation_words:\n",
    "        return word\n",
    "    return \"NOT_\"+word\n",
    "\n",
    "def handle_negation(comments):\n",
    "    sentences = nltk.sent_tokenize(comments)\n",
    "    modified_st=[]\n",
    "    for st in sentences:\n",
    "        allwords = nltk.word_tokenize(st)\n",
    "        modified_words=[]\n",
    "        if negated(allwords):\n",
    "            part_of_speech = nltk.tag.pos_tag(allwords,tagset='universal')\n",
    "            chunked = chunk_parser.parse(part_of_speech)\n",
    "            #print(\"---------------------------\")\n",
    "            #print(st)\n",
    "            for n in chunked:\n",
    "                if isinstance(n, nltk.tree.Tree):\n",
    "                    words = [pair[0] for pair in n.leaves()]\n",
    "                    #print(words)\n",
    "\n",
    "                    if n.label() == 'NegP' and negated(words):\n",
    "                        for i, (word, pos) in enumerate(n.leaves()):\n",
    "                            if (pos==\"ADV\" or pos==\"ADJ\" or pos==\"VERB\") and (word!=\"not\"):\n",
    "                                modified_words.append(prepend_not(word))\n",
    "                            else:\n",
    "                                modified_words.append(word)\n",
    "                    else:\n",
    "                         modified_words.extend(words)\n",
    "                else:\n",
    "                    modified_words.append(n[0])\n",
    "            newst =' '.join(modified_words)\n",
    "            #print(newst)\n",
    "            modified_st.append(newst)\n",
    "        else:\n",
    "            modified_st.append(st)\n",
    "    return \". \".join(modified_st)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    comments = text.encode('ascii', 'ignore')\n",
    "    comments = expand_contractions(comments)\n",
    "    comments = remove_url(comments)\n",
    "    comments = replace_all(comments, emodict)\n",
    "    comments = handle_negation(comments)\n",
    "\n",
    "    return  comments\n",
    "\n",
    "\n",
    "class SentimentData:\n",
    "    def __init__(self, text,rating):\n",
    "        self.text = text\n",
    "        self.rating =rating\n",
    "\n",
    "\n",
    "class SentiCR:\n",
    "    def __init__(self, algo=\"GBT\", training_data=None):\n",
    "        self.algo = algo\n",
    "        if(training_data is None):\n",
    "            self.training_data=self.read_data_from_oracle()\n",
    "        else:\n",
    "            self.training_data = training_data\n",
    "        self.model = self.create_model_from_training_data()\n",
    "\n",
    "\n",
    "    def get_classifier(self):\n",
    "        algo=self.algo\n",
    "\n",
    "        if algo==\"GBT\":\n",
    "            return GradientBoostingClassifier()\n",
    "        elif algo==\"RF\":\n",
    "            return  RandomForestClassifier()\n",
    "        elif algo==\"ADB\":\n",
    "            return AdaBoostClassifier()\n",
    "        elif algo ==\"DT\":\n",
    "            return  DecisionTreeClassifier()\n",
    "        elif algo==\"NB\":\n",
    "            return  BernoulliNB()\n",
    "        elif algo==\"SGD\":\n",
    "            return  SGDClassifier()\n",
    "        elif algo==\"SVC\":\n",
    "            return LinearSVC()\n",
    "        elif algo==\"MLPC\":\n",
    "            return MLPClassifier(activation='logistic',  batch_size='auto',\n",
    "            early_stopping=True, hidden_layer_sizes=(100,), learning_rate='adaptive',\n",
    "            learning_rate_init=0.1, max_iter=5000, random_state=1,\n",
    "            solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "            warm_start=False)\n",
    "        return 0\n",
    "\n",
    "    def create_model_from_training_data(self):\n",
    "        training_comments=[]\n",
    "        training_ratings=[]\n",
    "        print(\"Training classifier model..\")\n",
    "        for sentidata in self.training_data:\n",
    "            comments = preprocess_text(sentidata.text)\n",
    "            training_comments.append(comments)\n",
    "            training_ratings.append(sentidata.rating)\n",
    "\n",
    "        # discard stopwords, apply stemming, and discard words present in less than 3 comments\n",
    "        self.vectorizer = TfidfVectorizer(tokenizer=tokenize_and_stem, sublinear_tf=True, max_df=0.5,\n",
    "                                     stop_words=mystop_words, min_df=3)\n",
    "        X_train = self.vectorizer.fit_transform(training_comments).toarray()\n",
    "        Y_train = np.array(training_ratings)\n",
    "\n",
    "        #Apply SMOTE to improve ratio of the minority class\n",
    "        smote_model = SMOTE(ratio=0.5, random_state=None, k=None, k_neighbors=15, m=None, m_neighbors=15, out_step=.0001,\n",
    "                   kind='regular', svm_estimator=None, n_jobs=1)\n",
    "\n",
    "        X_resampled, Y_resampled=smote_model.fit_sample(X_train, Y_train)\n",
    "\n",
    "        model=self.get_classifier()\n",
    "        model.fit(X_resampled, Y_resampled)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def read_data_from_oracle(self):\n",
    "        workbook = open_workbook(\"oracle.xlsx\")\n",
    "        sheet = workbook.sheet_by_index(0)\n",
    "        oracle_data=[]\n",
    "        print(\"Reading data from oracle..\")\n",
    "        for cell_num in range(0, sheet.nrows):\n",
    "            comments=SentimentData(sheet.cell(cell_num, 0).value,sheet.cell(cell_num, 1).value)\n",
    "            oracle_data.append(comments)\n",
    "        return  oracle_data\n",
    "\n",
    "\n",
    "    def get_sentiment_polarity(self,text):\n",
    "        comment=preprocess_text(text)\n",
    "        feature_vector=self.vectorizer.transform([comment]).toarray()\n",
    "        sentiment_class=self.model.predict(feature_vector)\n",
    "        return sentiment_class\n",
    "\n",
    "    def get_sentiment_polarity_collection(self,texts):\n",
    "        predictions=[]\n",
    "        for text in texts:\n",
    "            comment=preprocess_text(text)\n",
    "            feature_vector=self.vectorizer.transform([comment]).toarray()\n",
    "            sentiment_class=self.model.predict(feature_vector)\n",
    "            predictions.append(sentiment_class)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "def ten_fold_cross_validation(dataset,ALGO):\n",
    "    kf = KFold(n_splits=10)\n",
    "\n",
    "    run_precision = []\n",
    "    run_recall = []\n",
    "    run_f1score = []\n",
    "    run_accuracy = []\n",
    "\n",
    "    count=1\n",
    "\n",
    "    #Randomly divide the dataset into 10 partitions\n",
    "    # During each iteration one partition is used for test and remaining 9 are used for training\n",
    "    for train, test in kf.split(dataset):\n",
    "        print(\"Using split-\"+str(count)+\" as test data..\")\n",
    "        classifier_model=SentiCR(algo=ALGO,training_data= dataset[train])\n",
    "\n",
    "        test_comments=[comments.text for comments in dataset[test]]\n",
    "        test_ratings=[comments.rating for comments in dataset[test]]\n",
    "\n",
    "        pred = classifier_model.get_sentiment_polarity_collection(test_comments)\n",
    "\n",
    "        precision = precision_score(test_ratings, pred, pos_label=-1)\n",
    "        recall = recall_score(test_ratings, pred, pos_label=-1)\n",
    "        f1score = f1_score(test_ratings, pred, pos_label=-1)\n",
    "        accuracy = accuracy_score(test_ratings, pred)\n",
    "\n",
    "        run_accuracy.append(accuracy)\n",
    "        run_f1score.append(f1score)\n",
    "        run_precision.append(precision)\n",
    "        run_recall.append(recall)\n",
    "        count+=1\n",
    "\n",
    "    return (mean(run_precision),mean(run_recall),mean(run_f1score),mean(run_accuracy))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Supervised sentiment classifier')\n",
    "\n",
    "    parser.add_argument('--algo', type=str,\n",
    "                        help='Classification algorithm', default=\"GBT\")\n",
    "\n",
    "\n",
    "    parser.add_argument('--repeat', type=int,\n",
    "                        help='Iteration count', default=100)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    ALGO = args.algo\n",
    "    REPEAT = args.repeat\n",
    "\n",
    "    print(\"Cross validation\")\n",
    "    print(\"Algrithm: \" + ALGO)\n",
    "    print(\"Repeat: \" + str(REPEAT))\n",
    "\n",
    "    workbook = open_workbook(\"oracle.xlsx\")\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    oracle_data = []\n",
    "\n",
    "    for cell_num in range(0, sheet.nrows):\n",
    "        comments = SentimentData(sheet.cell(cell_num, 0).value,sheet.cell(cell_num, 1).value)\n",
    "        oracle_data.append(comments)\n",
    "\n",
    "    random.shuffle(oracle_data)\n",
    "\n",
    "    oracle_data=np.array(oracle_data)\n",
    "\n",
    "    Precision = []\n",
    "    Recall = []\n",
    "    Fmean = []\n",
    "    Accuracy = []\n",
    "\n",
    "    for k in range (0,REPEAT):\n",
    "        print(\".............................\")\n",
    "        print(\"Run# {}\".format(k))\n",
    "        (precision, recall, f1score, accuracy)=ten_fold_cross_validation(oracle_data,ALGO)\n",
    "        Precision.append(precision)\n",
    "        Recall.append(recall)\n",
    "        Fmean.append(f1score)\n",
    "        Accuracy.append(accuracy)\n",
    "        print(\"Precision:\"+str(precision))\n",
    "        print(\"Recall:\" + str(recall))\n",
    "        print(\"F-measure:\" + str(f1score))\n",
    "        print(\"Accuracy:\" + str(accuracy))\n",
    "\n",
    "    ##########################\n",
    "    training = open(\"cross-validation-\" + ALGO + \".csv\", 'w')\n",
    "    training.write(\"Run,Algo,Precision,Recall,Fscore,Accuracy\\n\")\n",
    "\n",
    "    for k in range(0, REPEAT):\n",
    "        training.write(str(k) + \",\" + ALGO + \",\" + str(Precision[k]) + \",\" + str(Recall[k]) + \",\" +\n",
    "                       str(Fmean[k]) + \",\" + str(Accuracy[k]) + \"\\n\")\n",
    "    training.close()\n",
    "\n",
    "    print(\"-------------------------\")\n",
    "    print(\"Average Precision: {}\".format(mean(Precision)))\n",
    "    print(\"Average Recall: {}\".format(mean(Recall)))\n",
    "    print(\"Average Fmean: {}\".format(mean(Fmean)))\n",
    "    print(\"Average Accuracy: {}\".format(mean(Accuracy)))\n",
    "    print(\"-------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1c91b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
