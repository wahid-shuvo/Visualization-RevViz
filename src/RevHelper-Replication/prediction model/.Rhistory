importance=TRUE,
ntree=2000)
#developing the RevHelper-RF fit
fit <- randomForest(as.factor(className) ~
#all features considered
#.,
#textual features only
#  ReadingEase+ReadingEase.NL. +StopwordRatio+StopKeyRatio+
#  QuestionRatio+ CodeElementRatio+ConceptualSimilarity
#developer experience based features only
#AuthorCommitsFile+ CommittedTwice + TotalAuthoredCommits+ ReviewingTwice +ReviewedCommitsFile +	TotalReviewedCommits
#+ ReviewedPRs +ExtLibSimilarity
#optimal set of features
#ConceptualSimilarity
+
ReviewedCommitsFile + AuthorCommitsFile +ExtLibSimilarity +
TotalReviewedCommits+ ReviewedPRs+StopwordRatio,
data=train,
importance=TRUE,
ntree=2000)
print(fit)
#developing the RevHelper-RF fit
fit <- randomForest(as.factor(className) ~
#all features considered
.,
#textual features only
#  ReadingEase+ReadingEase.NL. +StopwordRatio+StopKeyRatio+
#  QuestionRatio+ CodeElementRatio+ConceptualSimilarity
#developer experience based features only
#AuthorCommitsFile+ CommittedTwice + TotalAuthoredCommits+ ReviewingTwice +ReviewedCommitsFile +	TotalReviewedCommits
#+ ReviewedPRs +ExtLibSimilarity
#optimal set of features
#ConceptualSimilarity
#+
#ReviewedCommitsFile + AuthorCommitsFile +ExtLibSimilarity +
#TotalReviewedCommits+ ReviewedPRs+StopwordRatio,
data=train,
importance=TRUE,
ntree=2000)
print(fit)
imp=importance(fit)
sumImp =sumImp + imp
#prediction model
Prediction <- predict(fit, test)
#calculate accuracy
cm=confusionMatrix(data=Prediction,
reference=test$class,
positive='u')
print(fit)
#print(fit)
imp=importance(fit)
sumImp =sumImp + imp
View(sumImp)
View(imp)
View(sumImp)
View(sumImp)
View(sumImp)
#load the model data
data=read.table(file="C:/Users/wahid/RevHelper-Replication-Package-MSR2017/evaluation/modeldata-training-and-testing.txt", header=TRUE)
clean
#replacing the NA values
cleandata=na.omit(data)
summary(cleandata)
#initialize the performance measures
sumAcc=0
sumPosPrec=0
sumPosRec=0
sumNegPrec=0
sumNegRec=0
sumImp=0
for(i in 200:209){
#setting seed points to ensure different randomization in each run
set.seed(i)
#shuffling the data
sdata=cleandata[sample(nrow(cleandata), nrow(cleandata)), ]
#divide the dataset into training and test set
n=nrow(sdata)
trainIndex = sample(1:n, size = round(0.65*n), replace=FALSE)
train=sdata[trainIndex,]
test=sdata[-trainIndex,]
#summary of train and test data if needed.
#summary(train)
#summary(test)
#developing the RevHelper-RF fit
fit <- randomForest(as.factor(className) ~
#all features considered
#.,
#textual features only
#  ReadingEase+ReadingEase.NL. +StopwordRatio+StopKeyRatio+
#  QuestionRatio+ CodeElementRatio+ConceptualSimilarity
#developer experience based features only
#AuthorCommitsFile+ CommittedTwice + TotalAuthoredCommits+ ReviewingTwice +ReviewedCommitsFile +	TotalReviewedCommits
#+ ReviewedPRs +ExtLibSimilarity
#optimal set of features
ConceptualSimilarity
+
ReviewedCommitsFile + AuthorCommitsFile +ExtLibSimilarity +
TotalReviewedCommits+ ReviewedPRs+StopwordRatio,
data=train,
importance=TRUE,
ntree=2000)
imp=importance(fit)
sumImp =sumImp + imp
#prediction model
Prediction <- predict(fit, test)
#calculate accuracy
cm=confusionMatrix(data=Prediction,
reference=test$class,
positive='u')
#show the confusion metrics if needed.
#print(cm)
#accumulating performance data from each run
sumAcc = sumAcc+  cm$overall['Accuracy']
sumPosPrec=sumPosPrec+ cm$byClass['Pos Pred Value']
sumPosRec=sumPosRec + cm$byClass['Sensitivity']
sumNegPrec=sumNegPrec + cm$byClass['Neg Pred Value']
sumNegRec=sumNegRec + cm$byClass['Specificity']
#showing performance from each run
print(c(cm$byClass['Pos Pred Value'],cm$byClass['Sensitivity'],cm$byClass['Neg Pred Value'],cm$byClass['Specificity'],cm$overall['Accuracy']) )
}
View(sumImp)
View(sumImp)
#load the model data
data=read.table(file="C:/Users/wahid/RevHelper-Replication-Package-MSR2017/evaluation/modeldata-training-and-testing.txt", header=TRUE)
#show a summary if needed
summary(data)
#replacing the NA values
cleandata=na.omit(data)
summary(cleandata)
#initialize the performance measures
sumAcc=0
sumPosPrec=0
sumPosRec=0
sumNegPrec=0
sumNegRec=0
#setting seed points to ensure different randomization in each run
set.seed(i)
for(i in 200:209){
#setting seed points to ensure different randomization in each run
set.seed(i)
#shuffling the data
sdata=cleandata[sample(nrow(cleandata), nrow(cleandata)), ]
#divide the dataset into training and test set
n=nrow(sdata)
trainIndex = sample(1:n, size = round(0.65*n), replace=FALSE)
train=sdata[trainIndex,]
test=sdata[-trainIndex,]
#summary of train and test data if needed.
#summary(train)
#summary(test)
#developing the RevHelper-RF fit
fit <- randomForest(as.factor(className) ~
#all features considered
#.,
#textual features only
#  ReadingEase+ReadingEase.NL. +StopwordRatio+StopKeyRatio+
#  QuestionRatio+ CodeElementRatio+ConceptualSimilarity
#developer experience based features only
#AuthorCommitsFile+ CommittedTwice + TotalAuthoredCommits+ ReviewingTwice +ReviewedCommitsFile +	TotalReviewedCommits
#+ ReviewedPRs +ExtLibSimilarity
#optimal set of features
ConceptualSimilarity
+
ReviewedCommitsFile + AuthorCommitsFile +ExtLibSimilarity +
TotalReviewedCommits+ ReviewedPRs+StopwordRatio,
data=train,
importance=TRUE,
ntree=2000)
imp=importance(fit)
sumImp =sumImp + imp
#prediction model
Prediction <- predict(fit, test)
#calculate accuracy
cm=confusionMatrix(data=Prediction,
reference=test$class,
positive='u')
#show the confusion metrics if needed.
#print(cm)
#accumulating performance data from each run
sumAcc = sumAcc+  cm$overall['Accuracy']
sumPosPrec=sumPosPrec+ cm$byClass['Pos Pred Value']
sumPosRec=sumPosRec + cm$byClass['Sensitivity']
sumNegPrec=sumNegPrec + cm$byClass['Neg Pred Value']
sumNegRec=sumNegRec + cm$byClass['Specificity']
#showing performance from each run
print(c(cm$byClass['Pos Pred Value'],cm$byClass['Sensitivity'],cm$byClass['Neg Pred Value'],cm$byClass['Specificity'],cm$overall['Accuracy']) )
}
#load the model data
data=read.table(file="C:/Users/wahid/RevHelper-Replication-Package-MSR2017/evaluation/modeldata-training-and-testing.txt", header=TRUE)
#show a summary if needed
summary(data)
#replacing the NA values
cleandata=na.omit(data)
summary(cleandata)
#initialize the performance measures
sumAcc=0
sumPosPrec=0
sumPosRec=0
sumNegPrec=0
sumNegRec=0
sumImp=0
for(i in 200:209){
#setting seed points to ensure different randomization in each run
set.seed(i)
#shuffling the data
sdata=cleandata[sample(nrow(cleandata), nrow(cleandata)), ]
#divide the dataset into training and test set
n=nrow(sdata)
trainIndex = sample(1:n, size = round(0.65*n), replace=FALSE)
train=sdata[trainIndex,]
test=sdata[-trainIndex,]
#summary of train and test data if needed.
#summary(train)
#summary(test)
#developing the RevHelper-RF fit
fit <- randomForest(as.factor(className) ~
#all features considered
.,
#textual features only
#  ReadingEase+ReadingEase.NL. +StopwordRatio+StopKeyRatio+
#  QuestionRatio+ CodeElementRatio+ConceptualSimilarity
#developer experience based features only
#AuthorCommitsFile+ CommittedTwice + TotalAuthoredCommits+ ReviewingTwice +ReviewedCommitsFile +	TotalReviewedCommits
#+ ReviewedPRs +ExtLibSimilarity
#optimal set of features
#ConceptualSimilarity
#+
#ReviewedCommitsFile + AuthorCommitsFile +ExtLibSimilarity +
#TotalReviewedCommits+ ReviewedPRs+StopwordRatio,
data=train,
importance=TRUE,
ntree=2000)
imp=importance(fit)
sumImp =sumImp + imp
#prediction model
Prediction <- predict(fit, test)
#calculate accuracy
cm=confusionMatrix(data=Prediction,
reference=test$class,
positive='u')
#show the confusion metrics if needed.
#print(cm)
#accumulating performance data from each run
sumAcc = sumAcc+  cm$overall['Accuracy']
sumPosPrec=sumPosPrec+ cm$byClass['Pos Pred Value']
sumPosRec=sumPosRec + cm$byClass['Sensitivity']
sumNegPrec=sumNegPrec + cm$byClass['Neg Pred Value']
sumNegRec=sumNegRec + cm$byClass['Specificity']
#showing performance from each run
print(c(cm$byClass['Pos Pred Value'],cm$byClass['Sensitivity'],cm$byClass['Neg Pred Value'],cm$byClass['Specificity'],cm$overall['Accuracy']) )
}
View(sumImp)
View(imp)
print(Prediction)
print(len(Prediction))
print(length(Prediction))
#calculate accuracy
cm=confusionMatrix(data=Prediction,
reference=test$class,
positive='u')
#calculate accuracy
cm=confusionMatrix(data=Prediction,
reference=test$className,
positive='u')
View(train)
View(train)
print(test$className)
print(Prediction)
#calculate accuracy
cm=confusionMatrix(data=Prediction,
reference=test$class,
positive='u')
table(Prediction)
predTable<-table(Prediction)
predTable<-table(test$className)
predTable<-table(Prediction)
predTable<-table(test$className)
#calculate accuracy
cm=confusionMatrix(data=Prediction,
reference=as.factor(test$class),
positive='u')
#show the confusion metrics if needed.
print(cm)
#accumulating performance data from each run
sumAcc = sumAcc+  cm$overall['Accuracy']
sumPosPrec=sumPosPrec+ cm$byClass['Pos Pred Value']
sumPosRec=sumPosRec + cm$byClass['Sensitivity']
sumNegPrec=sumNegPrec + cm$byClass['Neg Pred Value']
sumNegRec=sumNegRec + cm$byClass['Specificity']
#showing performance from each run
print(c(cm$byClass['Pos Pred Value'],cm$byClass['Sensitivity'],cm$byClass['Neg Pred Value'],cm$byClass['Specificity'],cm$overall['Accuracy']) )
for(i in 200:209){
#setting seed points to ensure different randomization in each run
set.seed(i)
#shuffling the data
sdata=cleandata[sample(nrow(cleandata), nrow(cleandata)), ]
#divide the dataset into training and test set
n=nrow(sdata)
trainIndex = sample(1:n, size = round(0.65*n), replace=FALSE)
train=sdata[trainIndex,]
test=sdata[-trainIndex,]
#summary of train and test data if needed.
#summary(train)
#summary(test)
#developing the RevHelper-RF fit
fit <- randomForest(as.factor(className) ~
#all features considered
.,
#textual features only
#  ReadingEase+ReadingEase.NL. +StopwordRatio+StopKeyRatio+
#  QuestionRatio+ CodeElementRatio+ConceptualSimilarity
#developer experience based features only
#AuthorCommitsFile+ CommittedTwice + TotalAuthoredCommits+ ReviewingTwice +ReviewedCommitsFile +	TotalReviewedCommits
#+ ReviewedPRs +ExtLibSimilarity
#optimal set of features
#ConceptualSimilarity
#+
#ReviewedCommitsFile + AuthorCommitsFile +ExtLibSimilarity +
#TotalReviewedCommits+ ReviewedPRs+StopwordRatio,
data=train,
importance=TRUE,
ntree=2000)
#print(fit)
imp=importance(fit)
sumImp =sumImp + imp
#prediction model
Prediction <- predict(fit, test)
predTable<-table(test$className)
print(test$className)
#calculate accuracy
cm=confusionMatrix(data=Prediction,
reference=as.factor(test$class),
positive='u')
#show the confusion metrics if needed.
print(cm)
#accumulating performance data from each run
sumAcc = sumAcc+  cm$overall['Accuracy']
sumPosPrec=sumPosPrec+ cm$byClass['Pos Pred Value']
sumPosRec=sumPosRec + cm$byClass['Sensitivity']
sumNegPrec=sumNegPrec + cm$byClass['Neg Pred Value']
sumNegRec=sumNegRec + cm$byClass['Specificity']
#showing performance from each run
print(c(cm$byClass['Pos Pred Value'],cm$byClass['Sensitivity'],cm$byClass['Neg Pred Value'],cm$byClass['Specificity'],cm$overall['Accuracy']) )
}
#showing average performance for 10 runs
print(sumPosPrec/10)
print(sumPosRec/10)
print(sumNegPrec/10)
print(sumNegRec/10)
print(sumAcc/10)
#loading the library
library(randomForest)
library(caTools)
library(e1071)
library(caret)
#load the model data
data=read.table(file="C:/Users/wahid/RevHelper-Replication/src/experiment/experiment-analysis/replication-package/evaluation/modeldata-training-and-testing.txt", header=TRUE)
#replacing the NA values
cleandata=na.omit(data)
summary(cleandata)
#initialize the performance measures
sumAcc=0
sumPosPrec=0
sumPosRec=0
sumNegPrec=0
sumNegRec=0
sumImp=0
for(i in 200:209){
#setting seed points to ensure different randomization in each run
set.seed(i)
#shuffling the data
sdata=cleandata[sample(nrow(cleandata), nrow(cleandata)), ]
#divide the dataset into training and test set
n=nrow(sdata)
trainIndex = sample(1:n, size = round(0.65*n), replace=FALSE)
train=sdata[trainIndex,]
test=sdata[-trainIndex,]
#summary of train and test data if needed.
#summary(train)
#summary(test)
#developing the RevHelper-RF fit
fit <- randomForest(as.factor(className) ~
#all features considered
.,
#textual features only
#  ReadingEase+ReadingEase.NL. +StopwordRatio+StopKeyRatio+
#  QuestionRatio+ CodeElementRatio+ConceptualSimilarity
#developer experience based features only
#AuthorCommitsFile+ CommittedTwice + TotalAuthoredCommits+ ReviewingTwice +ReviewedCommitsFile +	TotalReviewedCommits
#+ ReviewedPRs +ExtLibSimilarity
#optimal set of features
#ConceptualSimilarity
#+
#ReviewedCommitsFile + AuthorCommitsFile +ExtLibSimilarity +
#TotalReviewedCommits+ ReviewedPRs+StopwordRatio,
data=train,
importance=TRUE,
ntree=2000)
#print(fit)
imp=importance(fit)
sumImp =sumImp + imp
#prediction model
Prediction <- predict(fit, test)
predTable<-table(test$className)
print(test$className)
#calculate accuracy
cm=confusionMatrix(data=Prediction,
reference=as.factor(test$class),
positive='u')
#show the confusion metrics if needed.
print(cm)
#accumulating performance data from each run
sumAcc = sumAcc+  cm$overall['Accuracy']
sumPosPrec=sumPosPrec+ cm$byClass['Pos Pred Value']
sumPosRec=sumPosRec + cm$byClass['Sensitivity']
sumNegPrec=sumNegPrec + cm$byClass['Neg Pred Value']
sumNegRec=sumNegRec + cm$byClass['Specificity']
#showing performance from each run
print(c(cm$byClass['Pos Pred Value'],cm$byClass['Sensitivity'],cm$byClass['Neg Pred Value'],cm$byClass['Specificity'],cm$overall['Accuracy']) )
}
#show the confusion metrics if needed.
print(cm)
#accumulating performance data from each run
sumAcc = sumAcc+  cm$overall['Accuracy']
sumPosPrec=sumPosPrec+ cm$byClass['Pos Pred Value']
sumPosRec=sumPosRec + cm$byClass['Sensitivity']
sumNegPrec=sumNegPrec + cm$byClass['Neg Pred Value']
sumNegRec=sumNegRec + cm$byClass['Specificity']
#showing performance from each run
print(c(cm$byClass['Pos Pred Value'],cm$byClass['Sensitivity'],cm$byClass['Neg Pred Value'],cm$byClass['Specificity'],cm$overall['Accuracy']) )
#RevHelper Model
#loading the library
library(randomForest)
library(caTools)
library(e1071)
library(caret)
#load the model data
data=read.table(file="C:/Users/wahid/RevHelper-Replication/src/experiment/experiment-analysis/replication-package/evaluation/modeldata-training-and-testing.txt", header=TRUE)
#show a summary if needed
#summary(data)
#replacing the NA values
cleandata=na.omit(data)
summary(cleandata)
#initialize the performance measures
sumAcc=0
sumPosPrec=0
sumPosRec=0
sumNegPrec=0
sumNegRec=0
sumImp=0
for(i in 200:209){
#setting seed points to ensure different randomization in each run
set.seed(i)
#shuffling the data
sdata=cleandata[sample(nrow(cleandata), nrow(cleandata)), ]
#divide the dataset into training and test set
n=nrow(sdata)
trainIndex = sample(1:n, size = round(0.65*n), replace=FALSE)
train=sdata[trainIndex,]
test=sdata[-trainIndex,]
#summary of train and test data if needed.
#summary(train)
#summary(test)
#developing the RevHelper-RF fit
fit <- randomForest(as.factor(className) ~
#all features considered
.,
#textual features only
#  ReadingEase+ReadingEase.NL. +StopwordRatio+StopKeyRatio+
#  QuestionRatio+ CodeElementRatio+ConceptualSimilarity
#developer experience based features only
#AuthorCommitsFile+ CommittedTwice + TotalAuthoredCommits+ ReviewingTwice +ReviewedCommitsFile +	TotalReviewedCommits
#+ ReviewedPRs +ExtLibSimilarity
#optimal set of features
#ConceptualSimilarity
#+
#ReviewedCommitsFile + AuthorCommitsFile +ExtLibSimilarity +
#TotalReviewedCommits+ ReviewedPRs+StopwordRatio,
data=train,
importance=TRUE,
ntree=2000)
#print(fit)
imp=importance(fit)
sumImp =sumImp + imp
#prediction model
Prediction <- predict(fit, test)
predTable<-table(test$className)
print(test$className)
#calculate accuracy
cm=confusionMatrix(data=Prediction,
reference=as.factor(test$class),
positive='u')
#show the confusion metrics if needed.
print(cm)
#accumulating performance data from each run
sumAcc = sumAcc+  cm$overall['Accuracy']
sumPosPrec=sumPosPrec+ cm$byClass['Pos Pred Value']
sumPosRec=sumPosRec + cm$byClass['Sensitivity']
sumNegPrec=sumNegPrec + cm$byClass['Neg Pred Value']
sumNegRec=sumNegRec + cm$byClass['Specificity']
#showing performance from each run
print(c(cm$byClass['Pos Pred Value'],cm$byClass['Sensitivity'],cm$byClass['Neg Pred Value'],cm$byClass['Specificity'],cm$overall['Accuracy']) )
}
#showing average performance for 10 runs
print(sumPosPrec/10)
print(sumPosRec/10)
print(sumNegPrec/10)
print(sumNegRec/10)
print(sumAcc/10)
#showing average feature importance
myImp=sumImp/10
print(myImp)
